# Fast Response Service

âš¡ **Fast Response Service** is a REST API microservice that allows any application to send a prompt and input text to get quick responses generated by language models (LLM) such as AWS Bedrock Claude.

## ğŸš€ What is it for?

This service is ideal for tasks such as:
- **Summarizing texts** - Get concise summaries of long documents
- **Improving writing** - Enhance the quality of your texts
- **Rewriting content** - Adapt the tone and style of your writing
- **Generating automatic responses** - Create contextual replies
- **Any prompt-based task** - Complete flexibility for your needs

## ğŸ—ï¸ Tech Stack

- **FastAPI** (Python 3.13) - Modern and fast web framework
- **AWS Bedrock Claude** - State-of-the-art language model
- **Uvicorn** - High-performance ASGI server
- **Boto3** - Official AWS SDK for Python
- **Mangum** - AWS Lambda adapter

## ğŸ“ Project Structure

```
fast-response-service/
â”œâ”€â”€ .env                     # Environment variables
â”œâ”€â”€ .gitignore              # Files ignored by Git
â”œâ”€â”€ api_server.py           # Script to run the server
â”œâ”€â”€ main.py                 # Handler for AWS Lambda
â”œâ”€â”€ pyproject.toml          # Project configuration
â”œâ”€â”€ requirements.txt        # Project dependencies
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py    # API module initialization
â”‚   â”‚   â”œâ”€â”€ main.py        # Main FastAPI application
â”‚   â”‚   â”œâ”€â”€ models.py      # Pydantic models
â”‚   â”‚   â””â”€â”€ routes.py      # API endpoints
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ mining.py      # LLM interaction logic
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config/
â”‚       â”‚   â””â”€â”€ config_util.py  # AWS configuration
â”‚       â”œâ”€â”€ logger/
â”‚       â”‚   â””â”€â”€ logger_util.py  # Logging configuration
â”‚       â””â”€â”€ prompt/
â”‚           â”œâ”€â”€ prompt_prms.py  # Predefined PRMS prompts
â”‚           â””â”€â”€ prompt_star.py  # Predefined STAR prompts
â””â”€â”€ data/
    â””â”€â”€ logs/
        â””â”€â”€ app.log        # Application logs
```

## ğŸ“¦ Installation

### Prerequisites
- Python 3.13 or higher
- AWS account with Bedrock access
- Git

### 1. Clone the repository
```bash
git clone https://github.com/AllianceBioversityCIAT/ai-services.git
cd fast-response-service
```

### 2. Create virtual environment
```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Configure environment variables
Create a `.env` file in the project root:

```env
# AWS Bedrock Configuration
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID_BR=your_access_key_here
AWS_SECRET_ACCESS_KEY_BR=your_secret_key_here
```

## ğŸƒâ€â™‚ï¸ Running the Application

### Local development
```bash
# Option 1: Using the custom script
python api_server.py --reload

# Option 2: Using uvicorn directly
uvicorn app.api.main:app --reload --host 0.0.0.0 --port 8000
```

### Server options
```bash
python api_server.py --help
```

Available options:
- `--host`: Server IP (default: 0.0.0.0)
- `--port`: Server port (default: 8000)
- `--reload`: Auto-reload for development
- `--log-level`: Logging level (debug, info, warning, error, critical)

### Accessing the application
Once the server is started:
- **API Base:** http://localhost:8000
- **Swagger Documentation:** http://localhost:8000/docs
- **ReDoc Documentation:** http://localhost:8000/redoc
- **Health Check:** http://localhost:8000/health

## ğŸ“ API Endpoints

### 1. General Information
**GET** `/`
- **Description:** Basic information about the API
- **Response:**
```json
{
  "service": "Fast Response API",
  "version": "1.0.0",
  "description": "REST API for fast response generation using LLMs",
  "documentation": {
    "swagger_ui": "/docs",
    "redoc": "/redoc",
    "openapi_json": "/openapi.json"
  },
  "endpoints": {
    "POST /api/fast-response": "Generate a fast response from a prompt and text",
    "GET /health": "Health check endpoint"
  },
  "technology_stack": ["FastAPI", "AWS Bedrock", "Python 3.13"]
}
```

### 2. Health Check
**GET** `/health`
- **Description:** Service health status verification
- **Successful response:**
```json
{
  "status": "healthy",
  "service": "Fast Response API",
  "version": "1.0.0"
}
```

### 3. Generate Fast Response (Main Endpoint)
**POST** `/api/fast-response`

#### Request Body:
```json
{
  "prompt": "Summarize the following text in 3 lines.",
  "input_text": "Sustainable agriculture is key to rural development and food security. It involves practices that maintain long-term productivity while protecting the environment. These techniques include crop rotation, integrated pest management, and efficient water use."
}
```

#### Parameters:
- **`prompt`** (string, required): Instruction for the LLM model
  - Examples: "Summarize the text", "Improve the writing", "Translate to Spanish"
- **`input_text`** (string, required): Text to which the prompt will be applied

#### Successful response (200):
```json
{
  "prompt": "Summarize the following text in 3 lines.",
  "input_text": "Sustainable agriculture is key to...",
  "output": "Sustainable agriculture is fundamental to rural development and global food security. It's based on practices that maintain long-term productivity while protecting the ecosystem. Techniques include crop rotation, integrated pest management, and efficient water use.",
  "status": "success"
}
```

#### Error response (400):
```json
{
  "error": "Invalid parameters",
  "status": "error",
  "details": "Specific error description"
}
```

#### Error response (500):
```json
{
  "error": "Internal error",
  "status": "error",
  "details": "Internal error description"
}
```

## ğŸ§ª Usage Examples

### With cURL
```bash
# Example 1: Summarize text
curl -X POST "http://localhost:8000/api/fast-response" \
     -H "Content-Type: application/json" \
     -d '{
       "prompt": "Summarize the following text in 3 lines.",
       "input_text": "Sustainable agriculture is key to rural development..."
     }'

# Example 2: Improve writing
curl -X POST "http://localhost:8000/api/fast-response" \
     -H "Content-Type: application/json" \
     -d '{
       "prompt": "Improve the writing of the following paragraph:",
       "input_text": "Climate change is a very serious problem that affects everything."
     }'

# Example 3: Generate professional response
curl -X POST "http://localhost:8000/api/fast-response" \
     -H "Content-Type: application/json" \
     -d '{
       "prompt": "Generate a professional thank you response:",
       "input_text": "Client satisfied with our agricultural consulting service"
     }'
```

### With Python (requests)
```python
import requests
import json

url = "http://localhost:8000/api/fast-response"
headers = {"Content-Type": "application/json"}

data = {
    "prompt": "Translate the following text to Spanish:",
    "input_text": "Sustainable agriculture is fundamental for the future."
}

response = requests.post(url, headers=headers, data=json.dumps(data))
result = response.json()

print(f"Response: {result['output']}")
```

### With JavaScript (fetch)
```javascript
const response = await fetch('http://localhost:8000/api/fast-response', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    prompt: 'Create an attractive title for:',
    input_text: 'Article about benefits of organic agriculture'
  })
});

const result = await response.json();
console.log('Generated response:', result.output);
```

## ğŸ”§ Advanced Configuration

### Customize LLM Model
To change the Bedrock model, modify the file `app/llm/mining.py`:

```python
# Change the modelId in the invoke_model function
response = bedrock_runtime.invoke_model(
    modelId="us.anthropic.claude-3-haiku-20240307-v1:0",  # Faster model
    body=json.dumps(request_body),
    contentType="application/json",
    accept="application/json"
)
```

## ğŸš€ Deployment

### AWS Lambda
The project includes AWS Lambda support using Mangum:

```python
# main.py is already configured for Lambda
from mangum import Mangum
from app.api.main import app

handler = Mangum(app)
```

### Docker (optional)
```dockerfile
FROM python:3.13-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["uvicorn", "app.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## ğŸ› Troubleshooting

### Error: "AWS credentials not found"
1. Verify that AWS environment variables are configured
2. Ensure the `.env` file exists and is properly formatted
3. Check AWS Bedrock permissions in your account

### Error: "Model not available"
1. Confirm you have access to the Claude model in Bedrock
2. Verify the AWS region is correct
3. Request model access in the AWS Bedrock console

### Port in use error
```bash
# Change the port
python api_server.py --port 8001
```

### View detailed logs
```bash
# Run with debug logging
python api_server.py --log-level debug
```

Logs are saved in: `data/logs/app.log`

## ğŸ“Š Monitoring and Logs

### Log Structure
```
2024-01-15 10:30:45 - fast-response-service - INFO - routes.py:45 - âš¡ Generating fast response for prompt: Summarize text
2024-01-15 10:30:46 - fast-response-service - INFO - mining.py:25 - ğŸš€ Invoking the model...
```

### Automatic Health Check
```bash
# Monitoring script
#!/bin/bash
response=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8000/health)
if [ $response -eq 200 ]; then
    echo "âœ… Service running correctly"
else
    echo "âŒ Service with issues - Code: $response"
fi
```

## ğŸ¤ Contributing

1. Fork the project
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is under the MIT License. See `LICENSE` for more details.

---

**âš¡ Fast Response Service** - Generating intelligent responses at the speed of light